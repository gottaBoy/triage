第 01 篇：把整个组织类比成一个强化学习系统，讲了一级指标 MPS / MPD 怎么定义，相当于给组织定“目标函数 / loss”。
第 02 篇：从业务视角讲了指标背后需要哪些数据地基：1Hz 业务心跳、SLS + Flink 实时流、ODPS 数仓等。
这一篇，我们往下走一层，看一眼真正的地基工程：

车上跑一圈就是“每分钟 4GB”的原始数据，
这些东西到底是怎么写盘、怎么切片、怎么在被覆盖前留下来？
又是怎么映射成一个个 road case / bad case，
最后还能算出：到底是谁在用数据、用掉了多少流量、又带来了多少价值？
一、现实约束：一分钟 4GB，不可能全都往云上倒
L4 无人车在路上跑，数据量是什么级别？

多路摄像头视频（不同视角 / 分辨率 / 帧率）；
多线激光雷达点云、毫米波雷达、IMU、轮速、里程计等原始信号；
感知 / 预测 / PnC / 控制的各种中间结果和日志。
一台车跑一分钟，4GB 很正常：

1 小时 ≈ 240GB；
一天就是多个 TB；
一车几十 TB，车队规模一上来就是“数据海啸”。
三个现实是绕不过去的：

不可能全量实时上传所有原始数据：蜂窝流量、带宽、车端算力、云端存储都撑不住；
不能在车上“随便写文件”：否则事后想要留一小段数据，还得拆一个几百 MB 的大包，
IO 和算力都吃不消；
车载盘空间有限，大部分数据只能行车记录仪式循环覆盖，真正高价值的数据必须有办法被“认出来并保护住”。
所以设计车端日志体系的时候，我从一开始就有两个目标：

写盘那一刻就考虑以后怎么用，日志不是乱写；
逻辑上围绕 Case 管，物理上围绕小文件管，
让“要不要上传 / 要不要保留”退化成配置问题。
二、物理层：车端日志怎么切？20 秒纵切 + 类型横切
先说“盘上到底长什么样”。

2.1 纵向按时间：20 秒一片
所有内部日志原始形态都是连续时间流。
在车端落盘时，我们做了一个很简单却很关键的设计：

按 20 秒为单位做纵向切片。
[t0, t0+20s) 一片；
[t0+20s, t0+40s) 下一片；
……
好处有两个：

足够完整：大多数事件（一次急刹、一段画龙、一段停车不走）
前后几十秒就能说清楚；
粒度合适：
既不会太大导致上传/处理困难；
云端认出“这一片有价值”后，只要标记对应的 20 秒片段即可。
这里先埋一个伏笔：
真正做 Case ↔ 文件关联时，我们不会“死认 20 秒”，
而是用逻辑时间窗去覆盖物理时间窗，后面会展开。
2.2 横向按类型：同一时间片拆成几类文件
在每一个 20 秒时间片内部，再按“类型横切”拆包，大致分成几类：

（1）Microlog：高频姿态 + 底盘 + 控制的“行为黑匣子”
microlog 是专门为一级体感指标准备的一条高频 channel，
只关心一件事：车是怎么动的、是怎么被控制的。

典型内容：

姿态 / 动态：
车速（纵向 / 横向）、纵横向加速度；
航向角、航向角速度、横摆角速度等；
底盘 / CAN：
四轮轮速、转向机状态；
制动压力、底盘模式（行驶 / 驻车 / 紧急制动）；
控制：
算法下发油门、刹车、方向指令；
控制模式（自动 / 远程 / 手动、是否降级）。
特点：

采样频率高：典型 100Hz；
尽量接近原始 CAN / IMU 分辨率进行压缩，不做过度聚合；
不掺合场景语义（干线/园区/隧道/雪天），这些上云后和业务表 join 即可。
工程上：

microlog 是一条实时发出的 channel；
在车端用统一的 Python Trigger 框架订阅信号、打包二进制；
每 20 秒写成一个小二进制文件。
可以把它理解成车载行为黑匣子：
所有 MPS 指标的“事实底稿”都要靠它来还原。
（2）Mini log：最小“可看懂 + 可复现 + 可场景重建”的集合
mini log 的目标不是“什么都存”，而是：

**用尽量小的体积，支撑：
人/大模型看懂问题原因
仿真平台复现
通过场景重建复用场景
典型内容：

压缩后的前 / 后 / 左 / 右视频；
特殊场景的补充视图（如红绿灯裁剪图）；
各模块关键可视化中间结果：
感知障碍物框 / 类别；
预测轨迹；
PnC 规划轨迹线；
车道线、路缘等环境几何。
为了兼顾“可理解”和“体积小”，我们在车端就做了一些处理：

轨迹抽稀：
算法内部一条轨迹可能有 100 个点（点间距 0.2m）；
mini log 中主动抽到 ~20 个点：形状清晰、体积大幅下降。
多边形坐标压缩：
多边形 box / 3D box 内部大量 (x,y,z) 顶点；
落盘时量化 + zigzag + 可变长编码，把坐标差值压成很小的二进制块；
云端解包时还原出时序点列。
这样处理完之后，一段 mini log 本质上就是一段“有语义骨架的场景”：

有道路几何（车道线 / 路缘）；
有动态参与者（障碍物几何 + 轨迹）；
有自车规划轨迹。
在仿真平台里可以：

在高精地图上，用这些几何信息把当时的场景“重放”出来；
不一定每次都要依赖完整原始图像/点云，就能做多版本对比回放和回归。
这点很关键：
mini log 让“场景重建”成为可能，
可以用“重建后的场景”替代大量原始数据上传，减少成本。
（3）原始传感器数据：图像 / 点云等“重型货”
这部分是：

摄像头原始图像；
激光雷达 / 毫米波雷达原始点云 / 回波；
其他感知/定位任务需要的原始输入。
主要供：

感知模型训练 / 回归测试；
真值标注；
深度问题分析使用。
体量巨大，不可能默认全保，只在需要时按策略保留。

（4）其他算法中间结果 / 文本日志
包括：

模块间接口数据；
各类调试日志；
一些偶尔有用但不需要默认保留的通道。
按模块 / 功能分文件，按需拉取。

三、上传与解包：OSS + ODPS + Python UDF + SQL 粗筛 + 精判
microlog 这类二进制包，如果直接塞进数仓，成本会爆炸，我们的做法是：

3.1 OSS 存二进制，ODPS 存索引 + 帧级表
原始 microlog 二进制落到 OSS；
ODPS 的 ODS 表只存索引信息：
车辆 ID、时间段、文件大小；
对应的 OSS bucket + key。
后续处理时：

在 ODPS 中用 Python UDF 根据索引把 microlog 拉回来；
按协议解包成一帧一帧的记录（时间戳 + 速度 + 加速度 + 转角 + 控制指令等）；
写入一个帧级 microlog 表，支持 SQL 直接操作。
同时，为了节约资源：

microlog 上传时是压缩后的二进制包；
在 ODPS 里先用 Python UDF 解包成帧级 JSON / 结构化行，
再落到表里，方便后续 SQL 做粗筛。
3.2 先用 SQL 粗筛，再把少量候选交给 Trigger 精判
有了帧级表，不会第一步就让 Python 全表扫，而是：

a. 用 SQL 做“海选”：

保留自动驾驶状态；
保留速度大于某个阈值；
限定场地/城市/版本时间窗。
b. 再对候选结果做进一步 session 切分：

比如画龙：
先用 SQL 找所有大转向点；
按时间聚合成 session，前后各扩几秒，组成候选片段。
c. 最后把这些候选片段丢给 Python Trigger / UDF 精判：

看左右频繁交替、横向加速度模式等是否真的像“蛇形”；
真正命中的才算画龙的 road case。
3.3 必须“准实时”：1 小时内不认领，就会被覆盖
车上原始数据盘本质上是一个环形缓冲区，
只保留最近一段时间（比如 ~1 小时）的数据：

如果云端在这一小时内没有下发“保留这段数据”的指令，
这段数据就会像行车记录仪一样被新数据覆盖掉。
这决定了：

microlog 上云 + ODPS 解包 + Trigger 识别
必须在这个时间窗内跑完，达到“准实时”的效果，
才来得及告诉车：这几段 20 秒片段要保护起来。
四、逻辑层（1）：Road case = 按分钟编号的“场景母体 ID”
接下来从文件层往上抬一层，讲逻辑上的 Case 体系。

4.1 每车每分钟一个 road_case_id（统计主键）
在统计意义上，我们这么做：

一辆车在某一分钟，只会有一个 road_case_id。
比如车 A 在 2025-11-19 10:01 这一分钟，就生成一个唯一的 road_case_id；
10:02 再生成下一个；
这个 ID 是这一分钟逻辑片段的唯一身份，用在：
指标统计；
优先级管理；
成本归因；
后续 ROI 分析等。
4.2 Case 的时间窗不是死锁 60 秒，可以伸缩
非常重要的一点：

“每分钟一个 road_case_id”说的是统计上的主键，
不代表这个 case 的物理时间窗就必须等于 60 秒。
实际做法是：

默认情况下，case 的时间窗覆盖这一分钟内的关键时刻；
遇到复杂场景，会根据需要把时间窗往前/往后延伸：
比如“跟随慢速行人 40 秒”的 case，
为了回归和仿真，可能把时间窗扩到 90 秒：
前 20 秒：看进入场景过程；
中间 40 秒：跟随本身；
后 30 秒：退出场景。
有的 case 可能只需要 20 秒就能说清楚问题；
有的 case 会因为同一分钟内有多次触发而被“打补丁式”延长，
把多段时间合并进一个 case 时间窗。
但在指标统计层面，我们依然按“这分钟有没有 road case”来记，
方便按分钟统计 MPS / MPI。

4.3 指标按物理分钟切片：在不过度损失语义前提下，让 SQL 可维护
为了防止指标 SQL 变成无人敢动的怪兽，我们约定：

所有指标最终按照“物理世界的自然分钟”切片。
00:00:00 ~ 00:00:59 一片；
00:01:00 ~ 00:01:59 一片；
不搞奇技淫巧的滑动窗口。
规则大致是：

画龙 / 急刹等事件跨分钟时：
看事件起始时间落在哪一分钟，就算在哪一分钟；
一分钟内多次触发：
合并成一个 road case，统计侧只记 1 次。
核心原则就是：
在不损失太多业务语义的前提下，
把指标计算逻辑控制在“大家看得懂、改得动”的复杂度。
五、逻辑层（2）：Road case ↔ 物理文件——时间窗对齐与去重
有了 road_case_id 和物理小文件之后，中间需要一层“胶水”来把逻辑和物理串起来。

5.1 Case 和文件怎么绑定：按逻辑时间窗去“包住”物理文件
前面说过，车端落盘会按“20 秒 × 类型”去切文件，但在真实工程环境里，还有两个容易被忽略的现实：

不同类型的文件（microlog / mini / 原始传感器 / 中间结果），
由于缓存、线程调度、设备时间戳差异等原因，它们的起止时间并不一定严丝合缝；
相同类型不同通道的文件，也可能有几百毫秒、甚至一两秒的小错位。
如果在 Case ↔ 文件关联时，简单地用“第 N 个 20 秒片段”来粗暴对应，很容易出问题：

有的物理文件会覆盖不到整个 road case 的逻辑时间窗；
有的情况下，人/仿真在看某个 case 时会发现：
“microlog 时间刚好覆盖到了，但 mini 的视频少了前几秒”；
沟通和排查成本都很高。
所以在设计映射关系时，我们用的是 “逻辑时间窗 → 查询物理文件” 的方式：

每个 road_case_id 有一个明确的逻辑起始时间和结束时间（可伸缩）；
每个物理文件有自己的 file_id、start_ts、end_ts、file_type 等元信息；
在做 Case ↔ 文件映射时，用 case 的 [case_start, case_end] 去 Query 物理文件的 [file_start, file_end]，
保证选出的物理文件集合能够完整“包住”逻辑上的时间窗。
可以抽象成一张表（示意）：

case_file_mapping：
road_case_id（逻辑 Case 主键）
case_start_ts / case_end_ts（逻辑时间窗）
file_id（物理文件唯一 ID）
file_type（microlog / mini / raw / 中间结果…）
file_start_ts / file_end_ts（物理文件时间窗）
oss_key（物理文件在 OSS 上的路径）
priority（当前赋予这类文件的优先级）
关联逻辑大致是：

对于某个 road_case_id，我们知道它的逻辑时间窗是 [case_start, case_end]；
在文件元数据里，查找所有满足：
file_start_ts <= case_start_ts 且
file_end_ts >= case_end_ts
的文件集合，作为“完整覆盖 case 时间窗”的候选；
如果单个文件无法完整覆盖，则可以合并多个相邻文件，只要合并后时间范围能完整覆盖逻辑时间窗即可。
这样做的好处有两个：

对齐方式对人类和系统都更直观
和算法、运维、测试同学沟通时，可以直接说：
> “这个case 的时间是从 10:01:20 到 10:02:10，
> 你看到的所有 microlog/mini/原始数据，都是完全覆盖了这段时间。”
不会被“20 秒对齐假设”绑死
即便某个模块的日志文件是 19.8 秒、20.3 秒，甚至有轻微错位，
只要它的物理时间窗能覆盖逻辑时间窗，就可以被纳入这个 case 中；
反之，不完整覆盖的文件不会被误关联进来，避免后续分析时出现“时间对不齐”的诡异情况。
5.2 物理文件有唯一 file_id，多 Case 复用不重复上传
在物理层，每个文件都有一个全局唯一的 file_id，
它和 oss_key、file_start_ts、file_end_ts 等元信息一起，被管理在文件元数据表中。

这带来一个重要的特性：

同一个物理文件，可以被多个逻辑 road_case_id 引用，
但在上传和存储层只视为“一个文件”。
表现出来就是：

在 case_file_mapping 中，可能会出现多行记录引用同一个 file_id：
比如跨分钟的长场景被切成多个 road case，
它们的逻辑时间窗都落在同一个物理 mini log 文件内；
上传/同步系统以 file_id 为去重主键：
某个 file_id 被多个 case 命中，
实际上传时只传一次，存储时只存一份；
后续各个 case 只是“多条引用关系”而已。
这一点直接带来两方面收益：

进一步减少数据流量和重复存储
不会因为“这段数据被多个 case 用到了”就多次上传同一个文件；
Case 维度的复用不会带来物理层的爆炸。
2. 方便做精细化的成本与使用率分析

以 file_id 为单位，可以精确统计：
这个物理文件是被哪些 road_case_id 引用的；
在多少个场景（标注 / 训练 / 仿真）中被复用；
再结合 case_trigger_mapping，就能算出来：
> “某个 Trigger / 某个模块 / 某个挖掘需求，
> 实际上驱动了多少真实的文件上传与存储成本，
> 又产生了多少复用价值。”
六、优先级与保留策略：云端可配的“智能行车记录仪 + 黑匣子”
有了 case 和文件映射之后，就可以谈“哪些数据要保，保多久，保到什么粒度”了。

6.1 环形缓冲：默认可覆盖
基础行为是：

原始数据、部分 mini log、中间结果，
默认在车上以环形缓冲区形式存储，只保存最近约一小时；
没有被“点名保护”的时间片，会像行车记录仪一样被覆盖。
6.2 三个维度上配策略：时间片 × 数据类型 × 优先级
在云端配置里，我们把“保什么”拆成三块：

保哪些时间片？
以 road_case_id 为单位：
某个 road case 对应的时间窗是 [case_start, case_end]；
对长场景可以扩展到更长时间窗。
2. 在这些时间片里保哪些类型？

microlog（通常必保）；
mini log；
原始图像 / 点云；
某些模块中间结果；
……
可以按场景灵活组合，例如：

对于常规 MPS 异常分析：
只保 microlog + mini，足够复现问题；
对于极少见长尾场景：
再追加原始传感器数据，用于训练和精细标注。
3. 每类数据的优先级？

简单可以分成几档：

P0：黑匣子级（MPD 危险事件、重大资损、交警投诉等舆情事件）；
P1：高危近失误、一级指标明显异常的 road case；
P2：有价值的长尾问题、稀有场景；
P3：一般 case，只短期保留供调试。
所有这些都是云端可配置的，和车上算法版本解耦。
想改策略，不用发新版本，改配置下发即可。
6.3 真正的黑匣子：P0 级数据的保险箱
所谓“黑匣子”，在我们的体系里是：

一块单独划出来的存储区域，只放 MPD（Dangerous）级别的事件数据。
特性是：

不参加普通环形缓冲清理，不自动删除；
如果没有被上传到云端则不会被删除。
七、动态升优先级：接管语音、Trigger、LLM/VLM 打标统一走一条链路
前面讲到“优先级可配置”，这里再说一个很关键的点：

不管是接管时司机说的话、Trigger 自动打标，还是 LLM/VLM 自动打标，
最后都要汇入同一条“云端优先级决策链路”，
去动态调整 road case 的优先级和保留策略。
7.1 接管时的语音（ASR 文本）是一个“语义线索”
我们不会在车上单独开一个“语音 channel”，
而是在远程座舱接管时：

驾驶员会自然地说原因，例如：
“前面有人闯红灯”；
“前车突然加塞”；
“这个路口的灯坏了，一直不走”……
这些语音在远程座舱侧被 ASR 转成文本，
作为一条实时事件上报云端。
云端规则引擎或者LLM会在这些文本里：

匹配关键字 / 模式（闯红灯、逆行、施工、路口异常等）；
结合当时车况、场景、版本等信息判断“这个接管场景值不值得重视”。
一旦觉得“这是个非常值得关注的 case”，
就可以对对应的 road_case_id 做一件事：

把优先级从 P2 改成 P1/P0，
并在策略里要求：
保留 microlog + mini；
追加保留原始图像 / 点云；
必要时纳入黑匣子。
语音只是一个“语义信号”，真正起作用的是那条链路：

“云端识别出场景的重要性 → 提升 road case 优先级 → 修改车端保留策略”。
7.2 Trigger & LLM/VLM 自动打标：同样是“升优先级入口”
后续我们做的：

高召回规则 Trigger 自动打标；
LLM 对日志时间轴做 token 化 + 分类（自动归因）；
VLM 对图像 / 视频做自动场景打标；
本质上也都是：

在云端给 road case 多贴几层“语义标签”，
然后由一个统一的优先级决策器，
决定这个 case 未来是不是应该被当成高价值样本。
包括：

是不是应该追加保留原始传感器数据；
是不是应该进黑匣子；
后面标注 / 训练 / 仿真要不要优先用它。
所有这些入口，最终都会回到同一件事：

动态调整某个 road_case_id 的优先级，
从而改变那一段时间窗对应的小文件到底是保还是丢、保到什么粒度。
八、Road case & Bad case：从“场景母体”到“模块子问题”
现实中，一个 road case 里往往不止一个问题。
比如：

一次急刹车 road case：
既有感知障碍物误检 / 漏检；
又有 PnC 纵向控制过激；
甚至还有业务逻辑上的停车不走。
如果所有东西都挂在同一个 road_case_id 上：

统计上没法按模块拆分；
标注 / 训练 / 仿真也没法各模块各自维护问题集。
所以我们在 road case 上面，又加了一层：bad case。

8.1 Road case：场景母体
回顾一下 road case 的定位：

road_case_id = 代表这一段真实世界场景的“母体 ID”。
它主要管三件事：

MPS/MPI 级的体感指标；
场景重建与不同版本之间对齐；
统一的“场景时间窗”。
8.2 Bad case：从一个 road case 派生出的模块问题子集
对于每一个 road case，我们可以根据自动/人工的归因结果，
派生出多个 bad_case_id：

bad_case_id_1：感知团队的问题（误检 / 漏检 / 分类错误…）；
bad_case_id_2：PnC 团队的问题（刹车过激、轨迹难看…）；
bad_case_id_3：硬件/底盘问题（转向机变形、胎压异常导致画龙…）；
……
每个 bad case 可以有：

自己的时间窗；
自己的数据需求（感知侧要原始图像/点云，PnC 侧要 microlog+轨迹等）；
自己的闭环流程（标注 → 训练 / 仿真 → 回归）。
所有 bad_case_id 最终都指向原始 road_case_id，
这样既保留了“一段真实场景”的整体视角，又能按模块拆分问题。

8.3 统计与使用率分析：Case 真正“用没用起来”
有了：

road_case 表；
bad_case 表；
case_file_mapping（Case ↔ file_id）；
case_trigger_mapping（Case ↔ Trigger）；
你就可以回答很多过去回答不了的问题：

某个 Trigger 一年命中了多少 road case？
这些 case 派生出多少 bad case？
有多少 bad case 最终被导入了标注 / 训练 / 仿真？
这些场景有没有在新版本里被回归、指标有没有变好？
换句话说：

Case ID的逻辑映射 让“数据闭环”这件事，从“烧了多少存储和带宽”，
变成可以被量化的：

谁提的需求？
上传了多少数据？
真正用在了哪些模块？
对指标到底贡献了多少？
九、小结：地基打好了，上层玩什么都不慌
这一篇本质上只做了一件事：

把自动驾驶数据闭环的地基讲清楚：
从车端的物理文件，到云端的 road case / bad case / 优先级 / 使用率。
可以简单总结成几条：

设备物理层：
车端按“20 秒 × 类型”切成一堆小文件；
microlog 做高频行为黑匣子数据，mini log 做“可看懂 + 可复现 + 可场景重建”的最小集合；
原始传感器数据只在必要场景保留。
2. 上传与解包：

microlog 在 OSS 存二进制，ODPS 存索引 + 帧级表；
SQL 粗筛 + Python UDF 精判，
在“约 1 小时环形缓冲覆盖前”认出有价值的片段并下发保护指令。
3. Case 逻辑层：

每车每分钟一个 road_case_id 作为统计主键，时间窗可伸缩；
Case ↔ file_id 用逻辑时间窗去包住物理时间窗，支持多 Case 复用同一文件并按 file_id 去重；
Case ↔ Trigger 让数据上传成本可以精确归因到具体 Trigger 和团队。
4. 优先级与黑匣子：

一切保留策略都可以通过云端动态调整：
保哪些时间片；
保哪些类型；
赋予什么优先级；
MPD 级事件进黑匣子，永不自动覆盖。
5. 优先级调整统一接口：

接管语音 ASR 文本；
Trigger 自动打标；
LLM/VLM 自动打标；
最终都通过同一条“优先级决策链路”去调高 road case 的优先级，
决定哪些数据被保留下来、保到什么粒度。
6. Case用途的继续拆分映射：

Road case 是场景母体，负责对齐真实世界与版本；
Bad case 是模块子问题，负责对齐感知 / PnC / 硬件等各自的闭环；
Case ID 成为全局“数据身份证”，贯穿标注、训练、仿真，
也让“数据挖掘 / 数据闭环”这件事真正可以被量化。
正所谓基础不牢，地动山摇。这些年的摸爬滚打和各种试错，确实也走了不少弯路，趁这个机会把整个自动驾驶数据的基础设施梳理清楚，做出一套相对完整的顶层设计。这样未来在拓展性、可维护性上都会轻松很多。这套基础设施本质上是“物理世界 + AI”的通用模板，并不局限在无人车本身，放到其他机器人、无人机、乃至更多物理 AI 系统上也可以直接套用、向上扩展更多高阶能力。也希望这些实践经验，能给还在搭建数据闭环的同学们，带来一点点参考价值。