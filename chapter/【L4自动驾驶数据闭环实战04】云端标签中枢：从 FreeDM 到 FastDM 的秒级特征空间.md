如果把整条自动驾驶链路看成一个大模型，
一级指标是损失函数，秒级标签是特征空间，
而 FreeDM / FastDM / Trigger / LLM，就是在这个空间里挖数据、讲故事的工具。
前几篇写的是：目标函数怎么定（MPI / MPS / MPD）、指标怎么算、数据怎么上传和存储。
从这篇开始，我们进入到数据闭环里非常关键的一层——云端标签体系 + 数据挖掘框架。

这篇我想按时间线讲清楚几件事：

2020 年我先做的那套FreeDM（ODPS-SQL + Python UDF）是什么？
它为什么慢？它解决了什么，又暴露了什么问题？
为什么后来要演化出“秒级标签体系 + ADB 加速库 + FastDM”？
在这个基础上，怎么再叠加Trigger 框架 + LLM 语义检索 & 事件归因？
一、我想要一把真正能“挖数据”的铲子
做自动驾驶数据闭环，早期（大概 2018–2020 年）最强烈的痛点是：

明明云上堆着一大堆原始日志，但你想精准挖出某一类行为片段，
不是没有办法，而是非常“工程师向”、非常慢。
当时的需求大概是这种：

“帮我把最近一个月里所有疑似画龙的情况挖出来”；
“帮我找所有在园区里，因为前方行人导致的急刹场景”；
“帮我拿出一批窄路会车 + 有非机动车穿插的长尾样本回归”。
如果每次都从原始 Rosbag / Cyber / LCM / 自研日志格式开始写脚本：

一次性工程量巨大；
逻辑非常难复用；
算一次要跑半天。
所以 2020 年左右，我做了第一代云端挖数框架，起名字叫FreeDM—— Free Data Mining，意思是：

给你一把铲子，你可以“自由”地用 Python + SQL，
直接在原始库里挖出任何你想要的片段（一秒、多秒、整段 Session）。
这就是整个故事的起点。

二、FreeDM —— 纯 ODPS 的“自由挖数”时代
2.1 技术栈：OSS + ODPS-SQL + Python UDF + MapReduce 思想
在 FreeDM 里，底座是：

OSS
：存所有原始文件（microlog / mini log / Rosbag / Cyber / 各种内部格式）；
ODPS（MaxCompute）
：存这些文件的索引，以及挖数过程中的中间结果。
整个设计其实是借鉴了大数据处理里 MapReduce 的思想：

Map 阶段
：
把原始数据按时间 / 车辆打散到多个节点上；
在每个分片上用 Python UDF 做解析和秒级特征判断；
输出大量离散的(car_id, ts_second)命中点。
Reduce 阶段
：
在这些命中点基础上做聚合和拼接；
把离散的秒，组合成一段一段连续的行为片段（Session）；
然后再在每个 Session 上跑更复杂的 Python 逻辑（高级特征、统计、打标等）。

专利号：CN113900667A
这套数据挖掘体系当年还申请了个专利，对应到当年专利里的流程图，就是：

第一阶段：从数据库和对象存储里批量读取数据，跑code1做 Map；
第二阶段：对 Map 结果做聚合和拼接，跑code2做 Reduce。
最终输出的是一批可以直接消费的“数据时间片段”，用于标注、训练或评测。

2.2 优点：真的很“自由”
FreeDM 的优点非常明显：

原始数据只要接进 OSS + ODPS，就能用统一的方式访问；
行为特征逻辑全部用 Python 写，表达能力很强；
不管你要 1 秒片段、10 秒事件、1 分钟 Session，都能通过一套 MapReduce 式的代码搞定。
对当时的我来说，它解决了一个大问题：终于有了一把能在云上“挖数”的通用铲子。

2.3 痛点（一）：每次都要“从生米煮熟饭”，太慢了
但用了一年之后，几个问题也很清晰：

1.每次检索都要从头做“秒级筛选”

当时还没有“秒级标签表”的概念；
每次需求都是：
先写一段“秒级筛选逻辑”（Map 部分）；
再基于命中的秒拼 Session + 深度处理（Reduce 部分）；
结果是：每次挖一个新类 case，都要重新把那一个月的秒级特征扫一遍。
2.性能：一次完整跑下来经常要半小时级

对工程师来说能接受，对交互式探索来说太慢；
你想试几个不同的阈值 / 条件组合，就会变成“调参半小时起步”。
3.逻辑复用度不高

虽然都在 FreeDM 上写，但实质上还是“一次一套代码”；
相似需求之间很难完全复用，只能 copy+paste+改一改。
2.4 痛点（二）：数仓门槛高，真实用户还是“数据团队”
除了技术和性能上的问题，当时还有一个非常现实的情况：

1.数仓本身对研发同学不友好

数仓体系里，不同的数据分散在不同的 ODPS 表里；
想查一条链路，经常要连好几张表；
还涉及各种表权限申请、审批流程，体验并不好。
2.很多研发同学不愿意写 SQL / 云上 Python

写 SQL、写只能在云上执行的 Python UDF，本身就有门槛；
自动驾驶一线研发更愿意在本地写 C++/Python/仿真，不想去学一套“大数据语法”。
3.FreeDM 的实际主要用户，还是数据团队

即使有了 FreeDM 平台，真正上手写挖数逻辑的，大部分还是数据同学；
研发同学普遍的工作流还是：
> 提需求 → 发给数据团队 → 数据团队用 FreeDM 帮忙挖 → 过几小时/几天拿结果。
这就带来一个新的“思想矛盾”：

从“数据团队能挖到数据”到“一线研发 / QA / 运营自己会挖数据”，
中间还差一整层“易用性”。
所以从那个阶段开始，我就一直在想：

能不能做一个简单上手、可视化、鼠标点点点的挖数平台？
让一线研发同学，甚至不写代码的人，也能自己筛 Case、自己看数据？
这个念头没法在 FreeDM 本身里解决，只能去重构整套能力栈。这也就是后面FastDM出现的直接原动力之一。

接下来要讲的“秒级标签体系 + ADB 宽表 + FastDM”，本质上就是在解决：
既要算得动，又要查得快，还要用得起。

三、先把“秒级标签”算出来
3.1 关键思想：先建一个“每车每秒的特征空间”
从 FreeDM 的痛点反推出来，要做的事情其实就一句话：

把“每次查询都重新算”的秒级特征，
抽象成一个永久存在的“秒级标签表”。
也就是说，先不纠结 case 长什么样，
而是先把所有车的所有秒，全部映射到一个统一的结构化特征空间里。

这一步做完之后：

“秒级筛选”从“每次现算”变成“在一张表上查”；
FreeDM 的第一阶段可以被替换成“对这张秒级标签表做条件过滤”；
后面的 FastDM、Trigger、LLM，全都可以站在同一张“地板”上。
3.2 存储结构：竖表 + table 型扩展
在存储上，我最后落成了这套结构：

1）标签竖表（tall table）

字段大概是：

car_id：车辆 ID
ts_second：时间，精确到秒
tag_id：标签 ID
tag_name：标签名（人类可读）
tag_value_type
：值类型（enum/number/bool/string/array/table）
tag_value：标签值（按类型解释）
含义就是一句话：

“某辆车在某一秒，命中了什么标签，值是多少。”
2）table型标签：一对多的“子表入口”

对于“这一秒周围有哪些障碍物 / 车道线 / 地图要素”这类一对多信息，
用tag_value_type = 'table'表示：
竖表这一行只存“入口”；
具体明细在扩展表t_obstacles / t_lane_markings / t_map_features ...里。
这样：

要做复杂场景重建，就通过 table 标签去 join 对应扩展表；
要做大规模筛选，就在它之上派生布尔 / 数值标签（比如“2 米内行人数量 ≥ 3”）。
3.3 标签注册中心：先报户口，再允许生成
为了避免“野生标签”，我做了一个标签注册中心，所有标签必须先注册：

tag_id / tag_name / tag_value_type / 标签大类
；
生成方式（决定能不能下沉到车端 / Trigger / 仿真）
；
依赖的数据源（SLS / microlog / 地图 / 业务系统等）。
按生成方式分三类：

cloud_sql_only
：只在 ODPS 上通过 SQL 批处理生成的标签；
car_rule + cloud_rule
：既能云端回刷，又能下发车端规则 Trigger 的标签；
car_python + cloud_python
：既能在 ODPS 上跑 Python UDF，又能在车端沙箱里跑的复杂 Trigger 标签。
至此，“秒级标签是什么”和“标签怎么来”这两件事算是定清楚了。

四、ODPS + OSS —— 用离线增量批处理生成秒级标签
有了“秒级标签”的方向，就要解决“在哪里算、怎么算”的问题。

4.1 数据底座：OSS 存原始文件，ODPS 存索引
沿用 FreeDM 的底座：

OSS
：存原始 microlog / mini log / Rosbag / Cyber 等文件；
ODPS
：存这些文件的索引（路径、时间范围、车 ID 等）。
4.2 Python UDF：按需解析 OSS 文件 → JSON 明细
在 ODPS 的 Python UDF 中：

根据索引去拉 OSS 文件；
把二进制 microlog / mini log 解包成“每车每帧”的 JSON 明细；
这样标签生成逻辑就可以全部站在结构化 JSON上写。
4.3 标签生成 adapter：配置 → ODPS 批处理
把“标签定义”这个东西抽象出来，做成一层 adapter：

云端页面配置：标签依赖哪些字段、阈值是多少、逻辑是什么；
ODPS 侧 adapter：读取这些配置，自动翻译成 SQL + UDF 调用；
定时跑批：对新增的原始数据做离线增量计算，生成秒级标签记录，写入竖表。
4.4 增量 + 补算：让标签“随时可加、历史可补”
设计上我刻意保证两点：

1.增量计算

每次只对“上次处理时间之后”的新 OSS 文件跑标签生成；
2.按需补算

当新增一个标签时，可以指定时间范围，从历史 OSS 数据中“回刷”出这个标签；
不需要改原始日志，也不需要重新上传。
这样做完，第一代 FreeDM 里“每次现算秒级特征”的逻辑，就被抽象成了一个稳定存在的秒级标签表。

五、ADB 宽表+标签组合化检索 —— 从半小时到 1 分钟
秒级标签表有了，下一步要解决的是：

怎么在 1 分钟之内，让人类交互式地“玩标签筛选”
，
而不是每次都等一个大作业跑完。
5.1 竖表 → 宽表：每车每秒一行
在 ODPS 中，我先做了一步 pivot，把竖表变成“大宽表”：

主键：car_id + ts_second；
各种标签摊成列：
数值型：速度、加速度、距离、密度…
枚举型：天气、场景类型、任务类型…
布尔型：是否急刹、是否大转向、是否 road_case 窗口…
如果有 300 个标签，那就是一张 302 列的宽表。

这样，多标签组合筛选就变成了一句简单的 WHERE：

SELECT car_id, ts_second FROM tag_wide_table WHERE tag_weather = 'rain' AND tag_scene = '园区' AND tag_ped_dense IN ('high', 'very_high') AND tag_emergency_brake = TRUE;

5.2 同步到 ADB：把查询速度拉到 1 分钟以内
然后，把这张宽表同步到ADB（AnalyticDB）或类似的 OLAP 引擎上：

底层是 SSD 列存，适合各种多维条件过滤 & 聚合；
大量计算（标签生成）已经提前在 ODPS 离线做完；
在线只需要做简单过滤 + Session 聚合。
在这一步之前，FreeDM 纯跑在 ODPS 上，一次完整挖数经常要半小时。
有了 ADB 加速库之后，同样复杂度的条件组合 + Session 聚合，一般在 1 分钟以内就能出结果。

为了控制成本：

ADB 默认只保留最近一个月的数据作为加速层；
更久之前的：
在 ODPS 上做“慢查询”；
或者按需把某个历史月份同步一份到 ADB 临时加速。
至此，秒级标签 + ADB 宽表这块“地板”才真正铺完。

六、FastDM —— 秒级标签 + Session 聚合的加速引擎
有了秒级标签表 + ADB 宽表，下一步就是把第一代 FreeDM 里的那套“秒级筛选 + Session 聚合”平台化，于是就诞生了FastDM（Fast Data Management）。

6.1 FastDM 的定位
一句话概括：

FastDM = 基于秒级标签和宽表，
做“秒级筛选 + 简单 Session 聚合 + Case 生成”的快速数据搜索引擎。
它主要解决三件事：

把“秒级筛选”变成一个可视化配置问题，而不是一堆 SQL；
把“Session 聚合 + Case ID 管理”做成平台能力；
把“1 分钟内筛一个月数据”的能力，开放给所有人（算法 / QA / 运营）。
6.2 使用流程：选标签 → 出 Case
FastDM 的典型使用过程是：

1.选标签 + 配条件

在界面上勾选标签：天气、场景类型、是否急刹、行人密度等等；
给每个标签配上阈值 / 区间 / 枚举值。
2.后台拼 SQL → 在 ADB 上跑查询

系统自动把这些配置翻译成 SQL WHERE 条件，在宽表上执行；
得到命中的(car_id, ts_second)秒级命中点。
3.Session 聚合 → fastdm_case_id

按car_id + ts_second排序；
根据配置（最大间隔、最长时长）把连续命中的秒聚合成 Session，注意这一步也是通过SQL直接完成，利用 LAG + 窗口 SUM 把秒级命中拼成 Session（做过数仓的同学肯定知道我在说什么）；
为每个 Session 生成一个fastdm_case_id，标明该 Case 的来源是 FastDM：
case_type = 'fastdm'
；
car_id + start_ts + end_ts
；
一组命中的标签快照。
4.映射到底层文件 → 导出到标注 / 仿真 / 训练

用car_id + 时间窗去查第三篇文章里那套“Case ↔ 文件分片”映射；
选出需要的 microlog / mini log / 原始传感器数据；
直接一键导出到标注平台 / 仿真平台 / 训练集构建 pipeline。
在目前规模下，通常是：几秒到几十秒的配置时间 + 小于 1 分钟的查询时间，就能从一个月的数据里筛出想要的 Case。

6.3 FreeDM vs FastDM：从 1.0 退场，到 2.0 复活
到这一步，FreeDM 和 FastDM 的分工就变得很清晰：

FastDM
：
基于秒级标签 + ADB 宽表；
面向绝大多数“基于标签筛 Case + 简单时间聚合”的需求；
更偏“交互式搜索 + 平台能力”，一线研发 / QA / 运营、不懂 SQL 的同学，也可以靠“点点点”拿到自己想要的数据。
在 FastDM 成熟之后，其实发生过一段很真实的情况：

FreeDM 1.0 在生产环境里几乎“消失”了一阵子。
原因也很简单：
FastDM + 标签体系已经可以覆盖 80% 以上的挖数需求，
只有极少数“非常复杂的时序模式”还需要工程师手写 Python 去筛。

这时，一个新的问题浮现出来：

FreeDM 1.0 的 Python 逻辑只能跑在 ODPS 云端；
车端和仿真端还有各自一套完全不同的逻辑；
同一类行为，在云 / 车 / 仿真上往往有三份不完全一致的代码。
于是，后来我们在 Trigger 框架上，做了一次“FreeDM 2.0 化”的升级：

把原来 FreeDM 里“写 Python 在 Session 级精细筛选数据”的能力，
抽出来做成一套Trigger 框架；
这套 Python Trigger：
既可以在 ODPS 云端跑批
（继续承担 FreeDM 1.0 的角色）；
也可以下发到车端，在沙箱里跑
（做实时 / 准实时挖数）；
还可以直接嵌入仿真平台，做评测用的触发 & 评价逻辑。
所以我现在更倾向于这样理解：

FreeDM 1.0 = 纯云端的挖数框架，
FreeDM 2.0 = 基于 Trigger 的“三端统一挖数与评价框架”。
本篇主要讲的是标签体系 + FastDM 这一层的“地板”，
Trigger / FreeDM 2.0 会在后面的章节里单独拆开讲：
怎么统一云 / 车 / 仿真三端的代码，真正做到“挖数逻辑一次编写，到处运行”。